{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f582ef75-a6a1-40a6-91c5-ae077fa8c997",
   "metadata": {},
   "source": [
    "# Imports Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0f8ad37-81a5-42ba-90d3-236589ef529b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "import math\n",
    "\n",
    "from opencage.geocoder import OpenCageGeocode\n",
    "import folium\n",
    "from folium import plugins\n",
    "\n",
    "import json\n",
    "import urllib.request\n",
    "import urllib.error\n",
    "\n",
    "from datetime import datetime\n",
    "from datetime import date, timedelta\n",
    "from typing import List, Any, Dict\n",
    "\n",
    "import configparser\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4689a23-8967-4634-a5d3-d8a19e47b234",
   "metadata": {},
   "source": [
    "# Generic Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "495c3837-ed97-4145-ab97-9406fcb4bd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets user input variable values from a ini file\n",
    "def get_property(ini_file_name:str, section:str, key:str):\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read(ini_file_name)\n",
    "        \n",
    "    if config.has_section(section):\n",
    "      print(f'Config file has section [{section}]')\n",
    "    else:\n",
    "      print(f'Config file does not have section {section}, please check the name of section or api key file existence')\n",
    "    \n",
    "    key_value: str = config[section][key]\n",
    "    print(f'The property value for <{key}> is:\\n{key_value}\\n')\n",
    "    return key_value\n",
    "\n",
    "# Gets all the section names\n",
    "def get_property_sections_with_regex(ini_file_name:str, search_words:str) -> list:\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read(ini_file_name)\n",
    "    find_sections: list = []\n",
    "    \n",
    "    for section_name in config.sections():\n",
    "        if bool(re.search(search_words, section_name)):\n",
    "            find_sections.append(section_name)\n",
    "    \n",
    "    return find_sections\n",
    "\n",
    "# Gets all the best domains based on country code\n",
    "def get_all_keys_properties(ini_file_name:str, section:str) -> dict:\n",
    "    config = configparser.ConfigParser()\n",
    "    config.optionxform = str # to preserve the case\n",
    "    config.read(ini_file_name)\n",
    "    keys = list(config[section].keys())\n",
    "    \n",
    "    contry_domain: dict = {}\n",
    "    for key in keys:\n",
    "        print(f'key is <{key}> value is <{config[section][key]}>')\n",
    "        values: list = config[section][key].split(',')\n",
    "        for value in values:\n",
    "            contry_domain.update({value: key})\n",
    "    print(f'dict is <{contry_domain}>')\n",
    "    return contry_domain\n",
    "\n",
    "def convert_to_datetime(data:pd, col_names: list):\n",
    "    for col_name in col_names:\n",
    "        data[col_name] = pd.to_datetime(data[col_name])\n",
    "        \n",
    "def convert_to_date(data:pd, col_names: list):\n",
    "    for col_name in col_names:\n",
    "        data[col_name] = pd.to_datetime(data[col_name]).dt.date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b327b000-d16c-4ecb-8f43-caaca319ce2d",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff6b075c-2c7d-4867-aedd-a5387bfd5871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants that are used in the subsequent functions\n",
    "INI_FILE = 'cehub.ini'\n",
    "\n",
    "# Section names\n",
    "FILE_PATHS_SECTION = 'File_Paths'\n",
    "CEHUB_SECTION = 'CE_Hub'\n",
    "WEATHER_MERGE_ADD_COL = 'Weather_Merge_'\n",
    "# TODO: make this section scanable from the ini file rather than hardcode it\n",
    "CEHUB_PRECIPITATION_DOMAINS = 'CE_Hub_Precipitation_Domains'\n",
    "CEHUB_TEMPRETURE_DOMAINS = 'CE_Hub_Tempreture_Domains'\n",
    "CEHUB_WIND_DOMAINS = 'CE_Hub_Wind_Domains'\n",
    "\n",
    "#Weather merge column suffix\n",
    "DEFAULT = 'default'\n",
    "\n",
    "# Property names\n",
    "INPUT_FILE_DIR = 'input_file_dir'\n",
    "OUTPUT_FILE_DIR = 'output_file_dir'\n",
    "SOURCE_DATA_FILENAME = 'source_data_filename'\n",
    "SHEET_NAME = 'excel_sheet_name'\n",
    "API_KEY = 'api_key'\n",
    "\n",
    "START_DATE_PROP ='start_date'\n",
    "END_DATE_PROP ='end_date'\n",
    "START_DATE_OFFSET = 'start_date_offset'\n",
    "END_DATE_OFFSET = 'end_date_offset'\n",
    "\n",
    "FROM_DATE = 'from_date'\n",
    "FROM_DATE_OFFSET = 'from_date_offset'\n",
    "TO_DATE = 'to_date'\n",
    "TO_DATE_OFFSET = 'to_date_offset'\n",
    "\n",
    "#CE Hub URL\n",
    "CEHUB_URL = 'http://my.meteoblue.com/dataset/query?apikey='\n",
    "\n",
    "#CE Hub data domains\n",
    "DOMAIN_NEMSGLOBAL = 'NEMSGLOBAL'\n",
    "DOMAIN_ERA5 = 'ERA5'\n",
    "DOMAIN_ERA5T = 'ERA5T'\n",
    "DOMAIN_CPCGBAUS = 'CPCGBAUS'\n",
    "DOMAIN_CHIRPS2 = 'CHIRPS2'\n",
    "DOMAIN_SOILGRIDS2 = 'SOILGRIDS2'\n",
    "\n",
    "#Weather codes\n",
    "TEMP = 11 #Is this short for tempreture?\n",
    "PRECIPITATION = 61\n",
    "HUMIDITY = 52\n",
    "WIND_SPEED = 32\n",
    "WIND_DIRECTION = 735\n",
    "CLOUDS_TOTAL = 71\n",
    "CLOUDS_HIGH = 75\n",
    "CLOUDS_MEDIUM = 74\n",
    "CLOUDS_LOW = 73\n",
    "SUNSHINE_DURATION = 191\n",
    "SHORTWAVE_RADIATION_TOTAL = 204\n",
    "SHORTWAVE_RADIATION_DIRECT = 258\n",
    "SHORTWAVE_RADIATION_DIFFUSE = 256\n",
    "EVAPOTRANSPIRATION = 261\n",
    "SOIL_TEMP = 85\n",
    "SOIL_MOISTURE = 144\n",
    "VAPPRESS_DEFICIT = 56\n",
    "UV_MEAN = 721\n",
    "#Weather level\n",
    "LVL_2M_ELV_CORRECTED = '2 m elevation corrected'\n",
    "LVL_2M_ABV_GND = '2 m above gnd'\n",
    "LVL_SFC = 'sfc'\n",
    "LVL_HIGH_CLD_LAY = 'high cld lay'\n",
    "LVL_MID_CLD_LAY = 'mid cld lay'\n",
    "LVL_LOW_CLD_LAY = 'low cld lay'\n",
    "LVL_10CM_DOWN = '0-10 cm down'\n",
    "LVL_10M_ABV_GND = '10 m above gnd'\n",
    "\n",
    "#Soil codes\n",
    "BULK_DENSITY = 808\n",
    "CATION_EXCHANGE_CAPACITY = 809\n",
    "CLAY_CONTENT_MASS_FRACTION = 803\n",
    "COARSE_FRAGMENTS_VOLUMETRIC_FRACTION = 807\n",
    "ORGANIC_CARBON_CONTENT = 811\n",
    "ORGANIC_CARBON_DENSITY = 838\n",
    "ORGANIC_CARBON_STOCKS = 837\n",
    "SAND_CONTENT_MASS_FRACTION = 805\n",
    "SILT_CONTENT_MASS_FRACTION = 804\n",
    "TOTAL_NITROGEN_CONTENT = 817\n",
    "PH_IN_H20 = 812\n",
    "#Soil level\n",
    "LVL_AGGREGATE = 'aggregated'\n",
    "LVL_30 = '0-30 cm'\n",
    "START_DEPTH_0 = 0\n",
    "END_DEPTH_30 = 30\n",
    "END_DEPTH_60 = 60\n",
    "\n",
    "#time resolution\n",
    "TIME_RESOLUTION_DAILY = 'daily'\n",
    "TIME_RESOLUTION_HOURLY = 'hourly'\n",
    "\n",
    "# Weather REST Request JSON Keys\n",
    "MAX = 'max'\n",
    "MIN = 'min'\n",
    "MEAN = 'mean'\n",
    "SUM = 'sum'\n",
    "\n",
    "# CE Hub REST Response JSON Keys\n",
    "DOMAIN = 'domain'\n",
    "TIME_INTERVALS = 'timeIntervals'\n",
    "GEOMETRY = 'geometry'\n",
    "COORDINATES = 'coordinates'\n",
    "LOCATION_NAMES = 'locationNames'\n",
    "CODES = 'codes'\n",
    "VARIABLE = 'variable'\n",
    "DATA_PER_TIME_INTERVAL = 'dataPerTimeInterval'\n",
    "DATA = 'data'\n",
    "AGGREGATION = 'aggregation'\n",
    "START_DEPTH = 'startDepth'\n",
    "END_DEPTH = 'endDepth'\n",
    "UNIT = 'unit'\n",
    "LEVEL = 'level'\n",
    "\n",
    "#Corn column names and shared with Weather data\n",
    "TRIAL = 'Trial'\n",
    "LATITUDE = 'Latitude'\n",
    "LONGITUDE = 'Longitude'\n",
    "ALTITUDE = 'Altitude'\n",
    "DATES = 'Dates'\n",
    "COUNTRY_CODE = 'Country_Code'\n",
    "\n",
    "#For BITS data preprocessing and Merging with CEHub data\n",
    "START_DATE_COLUMN = 'Start_Date'\n",
    "END_DATE_COLUMN = 'End_Date'\n",
    "\n",
    "#Dates Columns\n",
    "PLANT_DATE = 'Plant_Date'\n",
    "EMERGENCE_DATE = 'Emergence_Date'\n",
    "ASSMT_DATE = 'Assmt_Date'\n",
    "FIRST_APPL_DATE_TRT = 'First_Appl_Date_Trt'\n",
    "FIRST_APPL_DATE_TRIAL = 'First_Appl_Date_Trial'\n",
    "MOST_RECENT_APPL_DATE_TRT = 'Most_Recent_Appl_Date_Trt'\n",
    "MOST_RECENT_APPL_DATE_TRIAL = 'Most_Recent_Appl_Date_Trial'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddfaf5b",
   "metadata": {},
   "source": [
    "# User Input Properties Loading From INI File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c1817448-b09a-49e8-80e2-f496e699169a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config file has section [File_Paths]\n",
      "The property value for <input_file_dir> is:\n",
      "C:\\example\\fields\\cehub\\input\n",
      "\n",
      "Config file has section [File_Paths]\n",
      "The property value for <output_file_dir> is:\n",
      "C:\\example\\fields\\cehub\\output\n",
      "\n",
      "Config file has section [File_Paths]\n",
      "The property value for <source_data_filename> is:\n",
      "failed_trials.xlsx\n",
      "\n",
      "Config file has section [File_Paths]\n",
      "The property value for <excel_sheet_name> is:\n",
      "Sheet 1\n",
      "\n",
      "Config file has section [CE_Hub]\n",
      "The property value for <api_key> is:\n",
      "\n",
      "\n",
      "Config file has section [CE_Hub]\n",
      "The property value for <start_date_offset> is:\n",
      "-5\n",
      "\n",
      "Config file has section [CE_Hub]\n",
      "The property value for <end_date_offset> is:\n",
      "5\n",
      "\n",
      "key is <ERA5T> value is <US,BR,DEFAULT>\n",
      "dict is <{'US': 'ERA5T', 'BR': 'ERA5T', 'DEFAULT': 'ERA5T'}>\n",
      "key is <ERA5T> value is <US,BR,DEFAULT>\n",
      "dict is <{'US': 'ERA5T', 'BR': 'ERA5T', 'DEFAULT': 'ERA5T'}>\n",
      "key is <ERA5T> value is <BR,US,DEFAULT>\n",
      "dict is <{'BR': 'ERA5T', 'US': 'ERA5T', 'DEFAULT': 'ERA5T'}>\n"
     ]
    }
   ],
   "source": [
    "# Loads the directory where the input data file is\n",
    "input_file_dir = get_property(INI_FILE, FILE_PATHS_SECTION, INPUT_FILE_DIR)\n",
    "\n",
    "# Loads the directory that stores output weather/soil/merged data files\n",
    "output_file_dir = get_property(INI_FILE, FILE_PATHS_SECTION, OUTPUT_FILE_DIR)\n",
    "\n",
    "# Loads trial data file name\n",
    "source_data_filename = get_property(INI_FILE, FILE_PATHS_SECTION, SOURCE_DATA_FILENAME)\n",
    "\n",
    "# Load sheet name in a excel file\n",
    "excel_sheet_name = get_property(INI_FILE, FILE_PATHS_SECTION, SHEET_NAME)\n",
    "\n",
    "# Loads CE Hub API key and constructs the endpoint url with the key\n",
    "api_key = get_property(INI_FILE, CEHUB_SECTION, API_KEY)\n",
    "cehub_endpoint = CEHUB_URL + api_key\n",
    "\n",
    "# Loading start and end date column names and their offset values set by the user for CE Hub data retrieval\n",
    "# start_date_column = 'Start_Date'\n",
    "start_date_offset = int(get_property(INI_FILE, CEHUB_SECTION, START_DATE_OFFSET))\n",
    "end_date_offset = int(get_property(INI_FILE, CEHUB_SECTION, END_DATE_OFFSET))\n",
    "\n",
    "# Check if start_date_offset > 0 set to = 0, if end_date_offset < 0 set to 0, less than its original min/max date does not make sense!\n",
    "if start_date_offset > 0:\n",
    "    print(f'start_date_offset should be set to less than 0, use 0 now instead of {start_date_offset}')\n",
    "    start_date_offset = 0\n",
    "if end_date_offset < 0:\n",
    "    print(f'end_date_offset should be set to more than 0, use 0 now instead of {end_date_offset}')\n",
    "    end_date_offset = 0\n",
    "\n",
    "# Loading best domain for coutries\n",
    "precipitation_domains: dict = get_all_keys_properties(INI_FILE, CEHUB_PRECIPITATION_DOMAINS)\n",
    "tempreture_domains: dict = get_all_keys_properties(INI_FILE, CEHUB_TEMPRETURE_DOMAINS)\n",
    "wind_domains: dict = get_all_keys_properties(INI_FILE, CEHUB_WIND_DOMAINS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca45118d-074e-4e68-8a6f-4450fdd395d2",
   "metadata": {},
   "source": [
    "# Trial Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3de65295-014c-44a8-a882-829fe1c81492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 94652 entries, 0 to 94651\n",
      "Data columns (total 97 columns):\n",
      " #   Column                           Non-Null Count  Dtype  \n",
      "---  ------                           --------------  -----  \n",
      " 0   Master_Prt                       94652 non-null  object \n",
      " 1   Derived_Prt                      94652 non-null  object \n",
      " 2   Trial                            94652 non-null  object \n",
      " 3   Trial_Year                       94652 non-null  int64  \n",
      " 4   Country_Code                     94652 non-null  object \n",
      " 5   State_province_code              94548 non-null  object \n",
      " 6   State_province_name              94548 non-null  object \n",
      " 7   City                             38759 non-null  object \n",
      " 8   Site_Type                        71929 non-null  object \n",
      " 9   Syngenta_FieldScientist          92875 non-null  object \n",
      " 10  Trial_Placement                  93494 non-null  object \n",
      " 11  Latitude                         41979 non-null  float64\n",
      " 12  Longitude                        41844 non-null  float64\n",
      " 13  Cooperator                       40193 non-null  object \n",
      " 14  Exp_Design                       94652 non-null  object \n",
      " 15  Quality                          93571 non-null  object \n",
      " 16  Trial_Status                     94652 non-null  object \n",
      " 17  Assmt_Num                        94652 non-null  int64  \n",
      " 18  Last_Assmt_Flag                  94177 non-null  object \n",
      " 19  Plant_Date                       81679 non-null  object \n",
      " 20  Assmt_Date                       94620 non-null  object \n",
      " 21  Emergence_Date                   17408 non-null  object \n",
      " 22  First_Appl_Date_Trt              68426 non-null  object \n",
      " 23  First_Appl_Date_Trial            88860 non-null  object \n",
      " 24  Most_Recent_Appl_Date_Trt        68426 non-null  object \n",
      " 25  Most_Recent_Appl_Date_Trial      88860 non-null  object \n",
      " 26  DAP                              81647 non-null  float64\n",
      " 27  DAE                              17408 non-null  float64\n",
      " 28  DAF_Appl_Trial                   88860 non-null  float64\n",
      " 29  DAMR_Appl_Trial                  88860 non-null  float64\n",
      " 30  DAF_Appl_Trt                     68426 non-null  float64\n",
      " 31  DAMR_Appl_Trt                    68426 non-null  float64\n",
      " 32  Crop_Number                      94652 non-null  int64  \n",
      " 33  Crop_Code                        94652 non-null  object \n",
      " 34  Crop                             94652 non-null  object \n",
      " 35  Variety                          91305 non-null  object \n",
      " 36  Crop_Attributes                  23665 non-null  object \n",
      " 37  Crop_Min_Stage_At_Assmt          67197 non-null  object \n",
      " 38  Crop_Max_Stage_At_Assmt          67193 non-null  object \n",
      " 39  Pest_Number                      66439 non-null  float64\n",
      " 40  Pest_Code                        66439 non-null  object \n",
      " 41  Pest                             66439 non-null  object \n",
      " 42  Pest_Attributes                  5768 non-null   object \n",
      " 43  Pest_Min_Stage_At_Assmt          47140 non-null  object \n",
      " 44  Pest_Max_Stage_At_Assmt          47140 non-null  object \n",
      " 45  Assmt_Type                       94652 non-null  object \n",
      " 46  Part_Rated                       94652 non-null  object \n",
      " 47  Part_Rated_Role                  64 non-null     object \n",
      " 48  Unit_Reporting_Basis             94580 non-null  object \n",
      " 49  Trt_Num                          94652 non-null  int64  \n",
      " 50  Product_Names                    94652 non-null  object \n",
      " 51  Product_Rate_Unit                72456 non-null  object \n",
      " 52  AI_Names                         77602 non-null  object \n",
      " 53  AI_Rate_Unit                     65490 non-null  object \n",
      " 54  Form_Type                        72456 non-null  object \n",
      " 55  Appl_Schedule                    72456 non-null  object \n",
      " 56  Appl_Description                 21351 non-null  object \n",
      " 57  Method                           71858 non-null  object \n",
      " 58  Placement                        71754 non-null  object \n",
      " 59  Timing                           64491 non-null  object \n",
      " 60  Spray_Volume                     65590 non-null  object \n",
      " 61  Trt_Flag                         27497 non-null  object \n",
      " 62  FactorA                          58 non-null     object \n",
      " 63  FactorA_Level_Desc               58 non-null     object \n",
      " 64  FactorB                          58 non-null     object \n",
      " 65  FactorB_Level_Desc               58 non-null     float64\n",
      " 66  FactorC                          0 non-null      float64\n",
      " 67  FactorC_Level_Desc               0 non-null      float64\n",
      " 68  MITF_Code                        177 non-null    object \n",
      " 69  MITF_Level                       177 non-null    object \n",
      " 70  Plot_ID                          94652 non-null  int64  \n",
      " 71  Range                            72033 non-null  float64\n",
      " 72  Row                              72033 non-null  float64\n",
      " 73  Rep_Num                          94652 non-null  int64  \n",
      " 74  Trt_Label                        94652 non-null  object \n",
      " 75  SE_Name                          31395 non-null  object \n",
      " 76  Assmt_Label                      94652 non-null  object \n",
      " 77  Rep_Value                        94652 non-null  float64\n",
      " 78  Check_Mean_Value                 85132 non-null  float64\n",
      " 79  Trt_Mean_Value                   94177 non-null  float64\n",
      " 80  Check_PCT                        511 non-null    float64\n",
      " 81  Climate_Zone_Code                371 non-null    object \n",
      " 82  Climate_Zone_Decode              371 non-null    object \n",
      " 83  Trial_TrialDesignType            94652 non-null  object \n",
      " 84  Forced_Single_Factor             1574 non-null   float64\n",
      " 85  Harmonized_Rep_Value             94652 non-null  float64\n",
      " 86  Harmonized_Unit_Reporting_Basis  94580 non-null  object \n",
      " 87  ARTIFICIAL_POPULATION            65289 non-null  object \n",
      " 88  Crop_Min_Stage_At_Appl           50476 non-null  object \n",
      " 89  Crop_Max_Stage_At_Appl           50476 non-null  object \n",
      " 90  Crop_Majority_Stage_At_Appl      11501 non-null  float64\n",
      " 91  Crop_Scale_At_Appl               11846 non-null  object \n",
      " 92  Crop_Min_Height_At_Appl          50790 non-null  float64\n",
      " 93  Crop_Max_Height_At_Appl          50844 non-null  float64\n",
      " 94  Crop_Height_Unit_at_Appl         51160 non-null  object \n",
      " 95  Start_Date                       94620 non-null  object \n",
      " 96  End_Date                         94620 non-null  object \n",
      "dtypes: float64(23), int64(6), object(68)\n",
      "memory usage: 70.0+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Trial</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Country_Code</th>\n",
       "      <th>Assmt_Date</th>\n",
       "      <th>First_Appl_Date_Trt</th>\n",
       "      <th>First_Appl_Date_Trial</th>\n",
       "      <th>Most_Recent_Appl_Date_Trt</th>\n",
       "      <th>Most_Recent_Appl_Date_Trial</th>\n",
       "      <th>Plant_Date</th>\n",
       "      <th>Emergence_Date</th>\n",
       "      <th>Start_Date</th>\n",
       "      <th>End_Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US01AH12345</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>2000-05-09</td>\n",
       "      <td>2000-04-06</td>\n",
       "      <td>2000-04-06</td>\n",
       "      <td>2000-04-06</td>\n",
       "      <td>2000-04-06</td>\n",
       "      <td>2000-04-27</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2000-04-01</td>\n",
       "      <td>2000-05-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>US01AH12345</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>2000-05-09</td>\n",
       "      <td>2000-04-06</td>\n",
       "      <td>2000-04-06</td>\n",
       "      <td>2000-04-06</td>\n",
       "      <td>2000-04-06</td>\n",
       "      <td>2000-04-27</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2000-04-01</td>\n",
       "      <td>2000-05-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>US01AH12345</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>2000-05-09</td>\n",
       "      <td>2000-04-06</td>\n",
       "      <td>2000-04-06</td>\n",
       "      <td>2000-04-06</td>\n",
       "      <td>2000-04-06</td>\n",
       "      <td>2000-04-27</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2000-04-01</td>\n",
       "      <td>2000-05-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>US01AH12345</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>2000-05-09</td>\n",
       "      <td>2000-04-06</td>\n",
       "      <td>2000-04-06</td>\n",
       "      <td>2000-04-06</td>\n",
       "      <td>2000-04-06</td>\n",
       "      <td>2000-04-27</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2000-04-01</td>\n",
       "      <td>2000-05-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>US01AH12345</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>2000-05-09</td>\n",
       "      <td>2000-04-06</td>\n",
       "      <td>2000-04-06</td>\n",
       "      <td>2000-04-06</td>\n",
       "      <td>2000-04-06</td>\n",
       "      <td>2000-04-27</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2000-04-01</td>\n",
       "      <td>2000-05-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94647</th>\n",
       "      <td>USNFSZ12345</td>\n",
       "      <td>12.345</td>\n",
       "      <td>12.345</td>\n",
       "      <td>US</td>\n",
       "      <td>2017-07-11</td>\n",
       "      <td>2017-06-06</td>\n",
       "      <td>2017-05-08</td>\n",
       "      <td>2017-06-06</td>\n",
       "      <td>2017-06-06</td>\n",
       "      <td>2017-05-04</td>\n",
       "      <td>2017-05-12</td>\n",
       "      <td>2017-04-29</td>\n",
       "      <td>2017-07-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94648</th>\n",
       "      <td>USNFSZ12345</td>\n",
       "      <td>12.345</td>\n",
       "      <td>12.345</td>\n",
       "      <td>US</td>\n",
       "      <td>2017-07-11</td>\n",
       "      <td>2017-06-06</td>\n",
       "      <td>2017-05-08</td>\n",
       "      <td>2017-06-06</td>\n",
       "      <td>2017-06-06</td>\n",
       "      <td>2017-05-04</td>\n",
       "      <td>2017-05-12</td>\n",
       "      <td>2017-04-29</td>\n",
       "      <td>2017-07-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94649</th>\n",
       "      <td>USNFSZ12345</td>\n",
       "      <td>12.345</td>\n",
       "      <td>12.345</td>\n",
       "      <td>US</td>\n",
       "      <td>2017-07-11</td>\n",
       "      <td>2017-06-06</td>\n",
       "      <td>2017-05-08</td>\n",
       "      <td>2017-06-06</td>\n",
       "      <td>2017-06-06</td>\n",
       "      <td>2017-05-04</td>\n",
       "      <td>2017-05-12</td>\n",
       "      <td>2017-04-29</td>\n",
       "      <td>2017-07-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94650</th>\n",
       "      <td>USNFSZ12345</td>\n",
       "      <td>12.345</td>\n",
       "      <td>12.345</td>\n",
       "      <td>US</td>\n",
       "      <td>2017-07-11</td>\n",
       "      <td>2017-06-06</td>\n",
       "      <td>2017-05-08</td>\n",
       "      <td>2017-06-06</td>\n",
       "      <td>2017-06-06</td>\n",
       "      <td>2017-05-04</td>\n",
       "      <td>2017-05-12</td>\n",
       "      <td>2017-04-29</td>\n",
       "      <td>2017-07-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94651</th>\n",
       "      <td>USNFSZ12345</td>\n",
       "      <td>12.345</td>\n",
       "      <td>12.345</td>\n",
       "      <td>US</td>\n",
       "      <td>2017-07-11</td>\n",
       "      <td>2017-06-06</td>\n",
       "      <td>2017-05-08</td>\n",
       "      <td>2017-06-06</td>\n",
       "      <td>2017-06-06</td>\n",
       "      <td>2017-05-04</td>\n",
       "      <td>2017-05-12</td>\n",
       "      <td>2017-04-29</td>\n",
       "      <td>2017-07-16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>94652 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Trial   Latitude  Longitude Country_Code  Assmt_Date  \\\n",
       "0      US01AH0542000        NaN        NaN           US  2000-05-09   \n",
       "1      US01AH0542000        NaN        NaN           US  2000-05-09   \n",
       "2      US01AH0542000        NaN        NaN           US  2000-05-09   \n",
       "3      US01AH0542000        NaN        NaN           US  2000-05-09   \n",
       "4      US01AH0542000        NaN        NaN           US  2000-05-09   \n",
       "...              ...        ...        ...          ...         ...   \n",
       "94647  USNFSZ2002017  44.035137 -92.684181           US  2017-07-11   \n",
       "94648  USNFSZ2002017  44.035137 -92.684181           US  2017-07-11   \n",
       "94649  USNFSZ2002017  44.035137 -92.684181           US  2017-07-11   \n",
       "94650  USNFSZ2002017  44.035137 -92.684181           US  2017-07-11   \n",
       "94651  USNFSZ2002017  44.035137 -92.684181           US  2017-07-11   \n",
       "\n",
       "      First_Appl_Date_Trt First_Appl_Date_Trial Most_Recent_Appl_Date_Trt  \\\n",
       "0              2000-04-06            2000-04-06                2000-04-06   \n",
       "1              2000-04-06            2000-04-06                2000-04-06   \n",
       "2              2000-04-06            2000-04-06                2000-04-06   \n",
       "3              2000-04-06            2000-04-06                2000-04-06   \n",
       "4              2000-04-06            2000-04-06                2000-04-06   \n",
       "...                   ...                   ...                       ...   \n",
       "94647          2017-06-06            2017-05-08                2017-06-06   \n",
       "94648          2017-06-06            2017-05-08                2017-06-06   \n",
       "94649          2017-06-06            2017-05-08                2017-06-06   \n",
       "94650          2017-06-06            2017-05-08                2017-06-06   \n",
       "94651          2017-06-06            2017-05-08                2017-06-06   \n",
       "\n",
       "      Most_Recent_Appl_Date_Trial  Plant_Date Emergence_Date  Start_Date  \\\n",
       "0                      2000-04-06  2000-04-27            NaT  2000-04-01   \n",
       "1                      2000-04-06  2000-04-27            NaT  2000-04-01   \n",
       "2                      2000-04-06  2000-04-27            NaT  2000-04-01   \n",
       "3                      2000-04-06  2000-04-27            NaT  2000-04-01   \n",
       "4                      2000-04-06  2000-04-27            NaT  2000-04-01   \n",
       "...                           ...         ...            ...         ...   \n",
       "94647                  2017-06-06  2017-05-04     2017-05-12  2017-04-29   \n",
       "94648                  2017-06-06  2017-05-04     2017-05-12  2017-04-29   \n",
       "94649                  2017-06-06  2017-05-04     2017-05-12  2017-04-29   \n",
       "94650                  2017-06-06  2017-05-04     2017-05-12  2017-04-29   \n",
       "94651                  2017-06-06  2017-05-04     2017-05-12  2017-04-29   \n",
       "\n",
       "         End_Date  \n",
       "0      2000-05-14  \n",
       "1      2000-05-14  \n",
       "2      2000-05-14  \n",
       "3      2000-05-14  \n",
       "4      2000-05-14  \n",
       "...           ...  \n",
       "94647  2017-07-16  \n",
       "94648  2017-07-16  \n",
       "94649  2017-07-16  \n",
       "94650  2017-07-16  \n",
       "94651  2017-07-16  \n",
       "\n",
       "[94652 rows x 13 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loads trial data into a dataframe, the crop type can be corn, grape etc.\n",
    "# trial_df = pd.read_csv(f'{input_file_dir}{os.path.sep}{source_data_filename}')\n",
    "\n",
    "# Preprocessing reports produced by BioAnalytics\n",
    "trial_df = pd.read_excel(f'{input_file_dir}{os.path.sep}{source_data_filename}', sheet_name= excel_sheet_name)\n",
    "\n",
    "interested_dates_cols: list = [ASSMT_DATE, FIRST_APPL_DATE_TRT, FIRST_APPL_DATE_TRIAL, MOST_RECENT_APPL_DATE_TRT, MOST_RECENT_APPL_DATE_TRIAL, PLANT_DATE, EMERGENCE_DATE]\n",
    "joined_on_cols: list = [TRIAL, LATITUDE, LONGITUDE, COUNTRY_CODE]\n",
    "start_end_cols: list = [START_DATE_COLUMN, END_DATE_COLUMN]\n",
    "\n",
    "# Converts the date columns to datetime for calculation\n",
    "convert_to_datetime(trial_df, interested_dates_cols)\n",
    "\n",
    "# Calculate min and max dates from the dates_of_interest columns, and add them back to the dataframe\n",
    "# This date will be used to extract the CE Hub data.\n",
    "trial_df[START_DATE_COLUMN] = trial_df.apply(lambda x: min(x[interested_dates_cols]) + timedelta(days = start_date_offset), axis = 1)\n",
    "trial_df[END_DATE_COLUMN] = trial_df.apply(lambda x: max(x[interested_dates_cols]) + timedelta(days = end_date_offset), axis = 1)\n",
    "\n",
    "trial_df[START_DATE_COLUMN] = pd.to_datetime(trial_df[START_DATE_COLUMN]).dt.date\n",
    "trial_df[END_DATE_COLUMN] = pd.to_datetime(trial_df[END_DATE_COLUMN]).dt.date\n",
    "\n",
    "# Convert the rest of the date column to date as well, after the start and end dates are calculated\n",
    "convert_to_date(trial_df, interested_dates_cols)\n",
    "\n",
    "# Remove 'Unnamed' columns from the dataframe\n",
    "trial_df.drop(trial_df.columns[trial_df.columns.str.contains('Unnamed')], axis=1, inplace=True)\n",
    "\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "trial_df.info()\n",
    "trial_df.loc[:, joined_on_cols + interested_dates_cols + start_end_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "abdf8b8e-0ed1-4194-9f22-97316dc01f9d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trial_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9416/2962726329.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# If we have many rows, for each Trial we have one lat, lon\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrial_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrial_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mjoined_on_cols\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstart_end_cols\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjoined_on_cols\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mSTART_DATE_COLUMN\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mMIN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEND_DATE_COLUMN\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mMAX\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtrial_time\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'trial_df' is not defined"
     ]
    }
   ],
   "source": [
    "# If we have many rows, for each Trial we have one lat, lon\n",
    "trial_time = trial_df[joined_on_cols + start_end_cols].groupby(joined_on_cols).agg({START_DATE_COLUMN: MIN, END_DATE_COLUMN: MAX}).reset_index()\n",
    "trial_time.info()\n",
    "display(trial_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9bb87ca5-0921-4e04-9ee3-5e3f86b13ad7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Trial</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Country_Code</th>\n",
       "      <th>Start_Date</th>\n",
       "      <th>End_Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>037SRBR12345-00</td>\n",
       "      <td>12.345</td>\n",
       "      <td>12.345</td>\n",
       "      <td>BR</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>2020-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>037SRBR12345-01</td>\n",
       "      <td>12.345</td>\n",
       "      <td>12.345</td>\n",
       "      <td>BR</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>2020-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TK1234567-12</td>\n",
       "      <td>12.345</td>\n",
       "      <td>12.345</td>\n",
       "      <td>US</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>2018-12-31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Trial   Latitude   Longitude Country_Code  Start_Date    End_Date\n",
       "0  037SRBR12345-00  12.345000   12.345000           BR  2020-01-01  2020-12-31\n",
       "1  037SRBR12345-01  12.345000   12.345000           BR  2020-01-01  2020-12-31\n",
       "2     TK1234567-12  12.345000   12.345000           US  2018-01-01  2018-12-31"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Hack for trials with 400 502 responses.\n",
    "data = {TRIAL: ['037SRBR12345-00', '037SRBR12345-01', 'TK1234567-12'], LATITUDE: [12.345, 12.345, 12.345], LONGITUDE: [12.345, 12.345, 12.345], COUNTRY_CODE: ['BR', 'BR', 'US'], START_DATE_COLUMN: ['2020-01-01', '2020-01-01', '2018-01-01'], END_DATE_COLUMN: ['2020-12-31', '2020-12-31', '2018-12-31']}\n",
    "trial_time = pd.DataFrame.from_dict(data)\n",
    "display(trial_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3fef00",
   "metadata": {},
   "source": [
    "# Functions For CE Hub REST APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffc281d",
   "metadata": {},
   "source": [
    "## Data Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5cd2bc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds weather data query for CE Hub, the return value is a list, since the data we are interested in is in two domains\n",
    "def build_weather_data_query(country_code: str) -> list:\n",
    "    domain_precipitation = precipitation_domains.get(country_code, precipitation_domains.get(DEFAULT.upper()))\n",
    "    domain_temp = tempreture_domains.get(country_code, tempreture_domains.get(DEFAULT.upper()))\n",
    "    domain_wind = wind_domains.get(country_code, wind_domains.get(DEFAULT.upper()))\n",
    "    print(f'country <{country_code}> use precipitation domain <{domain_precipitation}>, tempreture domain <domain_temp>, wind <{domain_wind}>')\n",
    "    \n",
    "    weather_query = [{\n",
    "        \"domain\":DOMAIN_NEMSGLOBAL,\n",
    "        \"timeResolution\":TIME_RESOLUTION_DAILY,\n",
    "        \"codes\":[\n",
    "            {\"code\":HUMIDITY,\"level\":LVL_2M_ABV_GND,\"aggregation\":MAX},               #Humidity_Max\n",
    "            {\"code\":HUMIDITY,\"level\":LVL_2M_ABV_GND,\"aggregation\":MIN},               #Humidity_Min\n",
    "            {\"code\":HUMIDITY,\"level\":LVL_2M_ABV_GND,\"aggregation\":MEAN},              #Humidity_Mean\n",
    "            {\"code\":CLOUDS_TOTAL,\"level\":LVL_SFC,\"aggregation\":MEAN},                 #Clouds_Total\n",
    "            {\"code\":CLOUDS_HIGH,\"level\":LVL_HIGH_CLD_LAY,\"aggregation\":MEAN},         #Clouds_High\n",
    "            {\"code\":CLOUDS_MEDIUM,\"level\":LVL_MID_CLD_LAY,\"aggregation\":MEAN},        #Clouds_Medium\n",
    "            {\"code\":CLOUDS_LOW,\"level\":LVL_LOW_CLD_LAY,\"aggregation\":MEAN},           #Clouds_Low\n",
    "            {\"code\":SUNSHINE_DURATION,\"level\":LVL_SFC,\"aggregation\":SUM},             #Sunshine_Duration\n",
    "            {\"code\":SHORTWAVE_RADIATION_TOTAL,\"level\":LVL_SFC,\"aggregation\":MEAN},    #Shortwave_Radiation_Total\n",
    "            {\"code\":SHORTWAVE_RADIATION_DIRECT,\"level\":LVL_SFC,\"aggregation\":MEAN},   #Shortwave_Radiation_Direct\n",
    "            {\"code\":SHORTWAVE_RADIATION_DIFFUSE,\"level\":LVL_SFC,\"aggregation\":MEAN},  #Shortwave_Radiation_Diffuse\n",
    "            {\"code\":EVAPOTRANSPIRATION,\"level\":LVL_SFC,\"aggregation\":SUM},            #Evapotranspiration\n",
    "            {\"code\":SOIL_TEMP,\"level\":LVL_10CM_DOWN,\"aggregation\":MAX},               #Soil_Temp_Max\n",
    "            {\"code\":SOIL_TEMP,\"level\":LVL_10CM_DOWN,\"aggregation\":MIN},               #Soil_Temp_Min\n",
    "            {\"code\":SOIL_TEMP,\"level\":LVL_10CM_DOWN,\"aggregation\":MEAN},              #Soil_Temp_Mean\n",
    "            {\"code\":SOIL_MOISTURE,\"level\":LVL_10CM_DOWN,\"aggregation\":MAX},           #Soil_Moisture_Max\n",
    "            {\"code\":SOIL_MOISTURE,\"level\":LVL_10CM_DOWN,\"aggregation\":MIN},           #Soil_Moisture_Min\n",
    "            {\"code\":SOIL_MOISTURE,\"level\":LVL_10CM_DOWN,\"aggregation\":MEAN},          #Soil_Moisture_Mean\n",
    "            {\"code\":VAPPRESS_DEFICIT,\"level\":LVL_2M_ABV_GND,\"aggregation\":MAX},       #VapPress_Deficit_Max\n",
    "            {\"code\":VAPPRESS_DEFICIT,\"level\":LVL_2M_ABV_GND,\"aggregation\":MIN},       #VapPress_Deficit_Min\n",
    "            {\"code\":VAPPRESS_DEFICIT,\"level\":LVL_2M_ABV_GND,\"aggregation\":MEAN}       #VapPress_Deficit_Mean\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"domain\":domain_temp,\n",
    "        \"timeResolution\":TIME_RESOLUTION_DAILY,\n",
    "        \"codes\":[\n",
    "            {\"code\":TEMP,\"level\":LVL_2M_ELV_CORRECTED,\"aggregation\":MAX},            #Temp_Max\n",
    "            {\"code\":TEMP,\"level\":LVL_2M_ELV_CORRECTED,\"aggregation\":MIN},            #Temp_Min\n",
    "            {\"code\":TEMP,\"level\":LVL_2M_ELV_CORRECTED,\"aggregation\":MEAN}            #Temp_Mean\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"domain\":domain_precipitation,\n",
    "        \"timeResolution\":TIME_RESOLUTION_DAILY,\n",
    "        \"codes\":[\n",
    "            {\"code\":PRECIPITATION,\"level\":LVL_SFC,\"aggregation\":SUM}                 #Precipitation        \n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"domain\":domain_wind,\n",
    "        \"timeResolution\":TIME_RESOLUTION_DAILY,\n",
    "        \"codes\":[\n",
    "            {\"code\":WIND_SPEED,\"level\":LVL_10M_ABV_GND,\"aggregation\":MAX},            #Wind_Max\n",
    "            {\"code\":WIND_SPEED,\"level\":LVL_10M_ABV_GND,\"aggregation\":MIN},            #Wind_Min\n",
    "            {\"code\":WIND_SPEED,\"level\":LVL_10M_ABV_GND,\"aggregation\":MEAN},           #Wind_Mean\n",
    "            {\"code\":WIND_DIRECTION,\"level\":LVL_10M_ABV_GND}                           #Wind_Direction        \n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"domain\": DOMAIN_ERA5,\n",
    "        \"gapFillDomain\": None,\n",
    "        \"timeResolution\": TIME_RESOLUTION_HOURLY,\n",
    "        \"codes\": [\n",
    "            {\n",
    "                \"code\": UV_MEAN,                                                      #UV_Mean\n",
    "                \"level\": LVL_SFC\n",
    "            }\n",
    "        ],\n",
    "        \"transformations\": [\n",
    "            {\n",
    "                \"type\": \"aggregateDaily\",\n",
    "                \"aggregation\": MEAN\n",
    "            }\n",
    "        ]\n",
    "    }]\n",
    "    \n",
    "    return weather_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "291b21a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds soil data query for CE Hub, here the information is only in one domain\n",
    "def build_soil_query (start_depth: int, end_depth: int) -> dict :\n",
    "    soil_query = {\n",
    "                    \"domain\": DOMAIN_SOILGRIDS2,\n",
    "                    \"codes\": [\n",
    "                    {\n",
    "                        \"code\": BULK_DENSITY,\n",
    "                        \"level\": LVL_AGGREGATE,\n",
    "                        \"startDepth\": start_depth,\n",
    "                        \"endDepth\": end_depth\n",
    "                \n",
    "                    },\n",
    "                    {\n",
    "                        \"code\": CATION_EXCHANGE_CAPACITY,\n",
    "                        \"level\": LVL_AGGREGATE,\n",
    "                        \"startDepth\": start_depth,\n",
    "                        \"endDepth\": end_depth\n",
    "                    },\n",
    "                    {\n",
    "                        \"code\": CLAY_CONTENT_MASS_FRACTION,\n",
    "                        \"level\": LVL_AGGREGATE,\n",
    "                        \"startDepth\": start_depth,\n",
    "                        \"endDepth\": end_depth\n",
    "                    },\n",
    "                    {\n",
    "                        \"code\": COARSE_FRAGMENTS_VOLUMETRIC_FRACTION,\n",
    "                        \"level\": LVL_AGGREGATE,\n",
    "                        \"startDepth\": start_depth,\n",
    "                        \"endDepth\": end_depth\n",
    "                    },\n",
    "                    {\n",
    "                        \"code\": ORGANIC_CARBON_CONTENT,\n",
    "                        \"level\": LVL_AGGREGATE,\n",
    "                        \"startDepth\": start_depth,\n",
    "                        \"endDepth\": end_depth\n",
    "                    },\n",
    "                    {\n",
    "                        \"code\": ORGANIC_CARBON_DENSITY,\n",
    "                        \"level\": LVL_AGGREGATE,\n",
    "                        \"startDepth\": start_depth,\n",
    "                        \"endDepth\": end_depth\n",
    "                    },\n",
    "                    {\n",
    "                        \"code\": ORGANIC_CARBON_STOCKS,\n",
    "                         \"level\": LVL_30 # only organic carbon stock only have one depth\n",
    "                    },\n",
    "                    {\n",
    "                        \"code\": SAND_CONTENT_MASS_FRACTION,\n",
    "                        \"level\": LVL_AGGREGATE,\n",
    "                        \"startDepth\": start_depth,\n",
    "                        \"endDepth\": end_depth\n",
    "                    },\n",
    "                    {\n",
    "                        \"code\": SILT_CONTENT_MASS_FRACTION,\n",
    "                        \"level\": LVL_AGGREGATE,\n",
    "                        \"startDepth\": start_depth,\n",
    "                        \"endDepth\": end_depth\n",
    "                    },\n",
    "                    {\n",
    "                        \"code\": TOTAL_NITROGEN_CONTENT,\n",
    "                        \"level\": LVL_AGGREGATE,\n",
    "                        \"startDepth\": start_depth,\n",
    "                        \"endDepth\": end_depth\n",
    "                    },\n",
    "                    {\n",
    "                        \"code\": PH_IN_H20,\n",
    "                        \"level\": LVL_AGGREGATE,\n",
    "                        \"startDepth\": start_depth,\n",
    "                        \"endDepth\": end_depth\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "    return soil_query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e8f024-ac1b-4774-af06-99629cde317b",
   "metadata": {},
   "source": [
    "## JSON Request Payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f8859636-548e-471a-998b-76ceded4681d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds CE Hub data JSON payload, the queries are built by using functions from the last step\n",
    "def build_json_payload(lat, lon, trial, start_date, end_date, queries):    \n",
    "    PARAMS = {\n",
    "        \"units\":{\n",
    "            \"temperature\":\"CELSIUS\",\n",
    "            \"velocity\":\"KILOMETER_PER_HOUR\",\n",
    "            \"length\":\"metric\",\n",
    "            \"energy\":\"watts\"\n",
    "        },\n",
    "        \"geometry\":{\n",
    "            \"type\":\"MultiPoint\",\n",
    "            \"coordinates\":[\n",
    "                [\n",
    "                    lon, lat,\n",
    "                ]\n",
    "            ],\n",
    "            \"locationNames\":[\n",
    "                f\"{trial}\"\n",
    "            ]\n",
    "        },\n",
    "        \"format\":\"json\",\n",
    "        \"timeIntervals\":[\n",
    "            f\"{start_date}T+10:00\\/{end_date}T+10:00\"\n",
    "        ],\n",
    "        \"timeIntervalsAlignment\":\"none\",\n",
    "        \"queries\": queries\n",
    "    }\n",
    "    \n",
    "    return PARAMS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae68078a-c3a1-444e-afb2-18d68a0ac3d1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## REST Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a107bdbe-263a-4cb9-905a-e77f392fcabe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_cehub_data(endpoint, lat, lon, trial, start_date, end_date, queries):\n",
    "    json_response = []\n",
    "    URL = endpoint\n",
    "    PARAMS = build_json_payload(lat, lon, trial, start_date, end_date, queries)\n",
    "    print(f'Getting trial <{trial}> for date range from <{start_date}> to <{end_date}>')\n",
    "    \n",
    "    try:\n",
    "        response = urllib.request.Request(URL, json.dumps(PARAMS).encode(\"utf-8\"), headers={'Content-type': 'application/json', 'Accept': 'application/json'})\n",
    "        json_response = json.loads(urllib.request.urlopen(response).read())\n",
    "      \n",
    "        return json_response\n",
    "    \n",
    "    except ConnectionError as ce:\n",
    "        print(f'Got connection error with exception {ce}')\n",
    "        time.sleep(10) \n",
    "    except Exception as exe:\n",
    "        print(f'No coordinates was found for trail: {trial}, exception is {exe}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974ccfd3-7d25-4936-92df-a322ab3ad2f6",
   "metadata": {},
   "source": [
    "## JSON Response to Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1860aa1d-aea9-4146-b083-58e79a08366a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts weater data REST call response JSON to dictionary\n",
    "def convert_weather_json_to_dict (json_response:List[Any]) -> Dict :\n",
    "    \n",
    "    response_dict = {}\n",
    "    for i in range(len(json_response)):\n",
    "        json = json_response[i]\n",
    "\n",
    "        #geometry\n",
    "        geometry = json[GEOMETRY]\n",
    "        coordinates = geometry[COORDINATES][0]\n",
    "        response_dict[TRIAL] = geometry[LOCATION_NAMES][0]\n",
    "        response_dict[LATITUDE] = coordinates[0]\n",
    "        response_dict[LONGITUDE] = coordinates[1]\n",
    "        response_dict[ALTITUDE] = coordinates[2]\n",
    "        #dates\n",
    "        response_dict[DATES] = json[TIME_INTERVALS][0]#str(json[TIME_INTERVALS][0]).replace('T0000','')\n",
    "        #codes\n",
    "        codes = json[CODES]\n",
    "        \n",
    "        for j in range(len(codes)):\n",
    "            agg:str = str(codes[j][AGGREGATION])\n",
    "            unit:str = codes[j][UNIT]\n",
    "            response_dict[str(codes[j][VARIABLE]).replace(' ', '_')+ '_('+ ''.join([agg[0].upper(), agg[1:]]) + ')_(' + unit + ')'] = codes[j][DATA_PER_TIME_INTERVAL][0][DATA][0]\n",
    "   \n",
    "   \n",
    "    return response_dict\n",
    "\n",
    "# Converts soil data REST call response JSON to dictionary\n",
    "def convert_soil_json_to_dict (json_response:List[Any]) -> Dict :\n",
    "    \n",
    "    response_dict = {}\n",
    "    for i in range(len(json_response)):\n",
    "        json = json_response[i]\n",
    "\n",
    "        #geometry\n",
    "        geometry = json[GEOMETRY]\n",
    "        coordinates = geometry[COORDINATES][0]\n",
    "        response_dict[TRIAL] = geometry[LOCATION_NAMES][0]\n",
    "        response_dict[LATITUDE] = coordinates[0]\n",
    "        response_dict[LONGITUDE] = coordinates[1]\n",
    "        \n",
    "        #codes\n",
    "        codes = json[CODES]\n",
    "        for j in range(len(codes)):\n",
    "            column_name: str\n",
    "            unit: str = codes[j][UNIT]\n",
    "            if codes[j][LEVEL] == LVL_AGGREGATE:\n",
    "                start_depth: int = codes[j][START_DEPTH]\n",
    "                end_depth: int = codes[j][END_DEPTH]\n",
    "                column_name = str(codes[j][VARIABLE]).replace(' ', '_') + '_('+ str(start_depth) + '-'+ str(end_depth) +')_(' + unit + ')'\n",
    "            else:\n",
    "                column_name = str(codes[j][VARIABLE]).replace(' ', '_')+ '_('+ codes[j][LEVEL]+')_(' + unit + ')'\n",
    "                \n",
    "            response_dict[column_name] = codes[j][DATA_PER_TIME_INTERVAL][0][DATA][0]\n",
    "    \n",
    "    return response_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74327841",
   "metadata": {},
   "source": [
    "# GETs Data From CE Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e5c67e-9c25-40ba-a977-cfee0f06035b",
   "metadata": {},
   "source": [
    "## GET Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cfbb3b96-af2a-46f7-995e-4d0e88047407",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "country <BR> use precipitation domain <ERA5T>, tempreture domain <domain_temp>, wind <ERA5T>\n",
      "Getting trial <037SRBR12345-00> for date range from <2020-01-01> to <2020-12-31>\n",
      "country <BR> use precipitation domain <ERA5T>, tempreture domain <domain_temp>, wind <ERA5T>\n",
      "Getting trial <037SRBR12345-01> for date range from <2020-01-01> to <2020-12-31>\n",
      "country <US> use precipitation domain <ERA5T>, tempreture domain <domain_temp>, wind <ERA5T>\n",
      "Getting trial <TK1234567-12> for date range from <2018-01-01> to <2018-12-31>\n",
      "<0> trials has failed to retrive from CE Hub out of <3>\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1097 entries, 0 to 1096\n",
      "Data columns (total 35 columns):\n",
      " #   Column                                     Non-Null Count  Dtype  \n",
      "---  ------                                     --------------  -----  \n",
      " 0   Trial                                      1097 non-null   object \n",
      " 1   Latitude                                   1097 non-null   float64\n",
      " 2   Longitude                                  1097 non-null   float64\n",
      " 3   Altitude                                   1097 non-null   float64\n",
      " 4   Dates                                      1097 non-null   object \n",
      " 5   Relative_Humidity_(Max)_(%)                1097 non-null   int64  \n",
      " 6   Relative_Humidity_(Min)_(%)                1097 non-null   int64  \n",
      " 7   Relative_Humidity_(Mean)_(%)               1097 non-null   float64\n",
      " 8   Cloud_Cover_Total_(Mean)_(%)               1097 non-null   float64\n",
      " 9   Cloud_Cover_High_(Mean)_(%)                1097 non-null   float64\n",
      " 10  Cloud_Cover_Medium_(Mean)_(%)              1097 non-null   float64\n",
      " 11  Cloud_Cover_Low_(Mean)_(%)                 1097 non-null   float64\n",
      " 12  Sunshine_Duration_(Sum)_(min)              1097 non-null   float64\n",
      " 13  Shortwave_Radiation_(Mean)_(W/mÂ²)          1097 non-null   float64\n",
      " 14  Direct_Shortwave_Radiation_(Mean)_(W/mÂ²)   1097 non-null   float64\n",
      " 15  Diffuse_Shortwave_Radiation_(Mean)_(W/mÂ²)  1097 non-null   float64\n",
      " 16  Evapotranspiration_(Sum)_(mm)              1097 non-null   float64\n",
      " 17  Soil_Temperature_(Max)_(Â°C)                1097 non-null   float64\n",
      " 18  Soil_Temperature_(Min)_(Â°C)                1097 non-null   float64\n",
      " 19  Soil_Temperature_(Mean)_(Â°C)               1097 non-null   float64\n",
      " 20  Soil_Moisture_(Max)_(mÂ³/mÂ³)                1097 non-null   float64\n",
      " 21  Soil_Moisture_(Min)_(mÂ³/mÂ³)                1097 non-null   float64\n",
      " 22  Soil_Moisture_(Mean)_(mÂ³/mÂ³)               1097 non-null   float64\n",
      " 23  Vapor_Pressure_Deficit_(Max)_(hPa)         1097 non-null   float64\n",
      " 24  Vapor_Pressure_Deficit_(Min)_(hPa)         1097 non-null   float64\n",
      " 25  Vapor_Pressure_Deficit_(Mean)_(hPa)        1097 non-null   float64\n",
      " 26  Temperature_(Max)_(Â°C)                     1097 non-null   float64\n",
      " 27  Temperature_(Min)_(Â°C)                     1097 non-null   float64\n",
      " 28  Temperature_(Mean)_(Â°C)                    1097 non-null   float64\n",
      " 29  Precipitation_Total_(Sum)_(mm)             1097 non-null   float64\n",
      " 30  Wind_Speed_(Max)_(km/h)                    1097 non-null   float64\n",
      " 31  Wind_Speed_(Min)_(km/h)                    1097 non-null   float64\n",
      " 32  Wind_Speed_(Mean)_(km/h)                   1097 non-null   float64\n",
      " 33  Wind_Direction_Dominant_(None)_(Â°)         1097 non-null   float64\n",
      " 34  UV_Radiation_(Mean)_(W/mÂ²)                 1097 non-null   float64\n",
      "dtypes: float64(31), int64(2), object(2)\n",
      "memory usage: 300.1+ KB\n",
      "\n",
      "Modifing datetime from yyyymmhhT0000 to yyyy-mm-dd...\n"
     ]
    }
   ],
   "source": [
    "weather_df = pd.DataFrame()\n",
    "\n",
    "total_trial:int = len(trial_time)\n",
    "failed_trials: int = 0\n",
    "\n",
    "for i in range(total_trial):\n",
    "    weather_queries = build_weather_data_query(trial_time[COUNTRY_CODE][i])\n",
    "    # weather_queries = build_weather_data_query()\n",
    "    json_response = get_cehub_data(cehub_endpoint, trial_time[LATITUDE][i], trial_time[LONGITUDE][i], trial_time[TRIAL][i], trial_time[START_DATE_COLUMN][i], trial_time[END_DATE_COLUMN][i], weather_queries)\n",
    "    try:\n",
    "        response_dict = convert_weather_json_to_dict(json_response)\n",
    "        each_trial_df = pd.DataFrame(response_dict)\n",
    "        weather_df = weather_df.append(each_trial_df, ignore_index=True)\n",
    "    except Exception as exe:\n",
    "        print(f\"Not working for {trial_time[TRIAL][i]} and it failed with: {exe}\")\n",
    "        failed_trials += 1\n",
    "\n",
    "print(f'<{failed_trials}> trials has failed to retrive from CE Hub out of <{total_trial}>')\n",
    "weather_df.info()\n",
    "\n",
    "print(f'\\nModifing datetime from yyyymmhhT0000 to yyyy-mm-dd...')\n",
    "weather_df[DATES] = weather_df[DATES].str.replace(r'T0000', '')\n",
    "weather_df[DATES] = pd.to_datetime(weather_df[DATES].str[:8], format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2127dfb1",
   "metadata": {},
   "source": [
    "## GET Soil Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d1c555f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting trial <037SRBR12345-00> for date range from <2020-01-01> to <2020-12-31>\n",
      "Getting trial <037SRBR12345-01> for date range from <2020-01-01> to <2020-12-31>\n",
      "Getting trial <TK1234567-12> for date range from <2018-01-01> to <2018-12-31>\n",
      "<0> trials has failed to retrive from CE Hub out of <3>\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3 entries, 0 to 2\n",
      "Data columns (total 24 columns):\n",
      " #   Column                                                       Non-Null Count  Dtype  \n",
      "---  ------                                                       --------------  -----  \n",
      " 0   Trial                                                        3 non-null      object \n",
      " 1   Latitude                                                     3 non-null      float64\n",
      " 2   Longitude                                                    3 non-null      float64\n",
      " 3   Bulk_Density_(0-30)_(kg/mÂ³)                                  3 non-null      float64\n",
      " 4   Cation_Exchange_Capacity_(0-30)_(cmolc/kg)                   3 non-null      float64\n",
      " 5   Clay_Content_(0-2_micro_meter)_mass_fraction_(0-30)_(%)      3 non-null      float64\n",
      " 6   Coarse_Fragments_volumetric_fraction_(0-30)_(vol. %)         3 non-null      float64\n",
      " 7   Organic_Carbon_Content_(fine_earth_fraction)_(0-30)_(%)      3 non-null      float64\n",
      " 8   Organic_Carbon_Density_(0-30)_(kg/mÂ³)                        3 non-null      float64\n",
      " 9   Organic_Carbon_Stocks_(0-30 cm)_(kg/mÂ²)                      3 non-null      float64\n",
      " 10  Sand_content_(50-2000_micro_meter)_mass_fraction_(0-30)_(%)  3 non-null      float64\n",
      " 11  Silt_Content_(2-50_micro_meter)_mass_fraction_(0-30)_(%)     3 non-null      float64\n",
      " 12  Total_Nitrogen_Content_(0-30)_(g/kg)                         3 non-null      float64\n",
      " 13  pH_in_H2O_(0-30)_(pH)                                        3 non-null      float64\n",
      " 14  Bulk_Density_(0-60)_(kg/mÂ³)                                  3 non-null      float64\n",
      " 15  Cation_Exchange_Capacity_(0-60)_(cmolc/kg)                   3 non-null      float64\n",
      " 16  Clay_Content_(0-2_micro_meter)_mass_fraction_(0-60)_(%)      3 non-null      float64\n",
      " 17  Coarse_Fragments_volumetric_fraction_(0-60)_(vol. %)         3 non-null      float64\n",
      " 18  Organic_Carbon_Content_(fine_earth_fraction)_(0-60)_(%)      3 non-null      float64\n",
      " 19  Organic_Carbon_Density_(0-60)_(kg/mÂ³)                        3 non-null      float64\n",
      " 20  Sand_content_(50-2000_micro_meter)_mass_fraction_(0-60)_(%)  3 non-null      float64\n",
      " 21  Silt_Content_(2-50_micro_meter)_mass_fraction_(0-60)_(%)     3 non-null      float64\n",
      " 22  Total_Nitrogen_Content_(0-60)_(g/kg)                         3 non-null      float64\n",
      " 23  pH_in_H2O_(0-60)_(pH)                                        3 non-null      float64\n",
      "dtypes: float64(23), object(1)\n",
      "memory usage: 704.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "soil_df = pd.DataFrame()\n",
    "\n",
    "total_trial:int = len(trial_time)\n",
    "failed_trials: int = 0\n",
    "\n",
    "for i in range(len(trial_time)):\n",
    "    soil_queries = [build_soil_query(START_DEPTH_0, END_DEPTH_30), build_soil_query(START_DEPTH_0, END_DEPTH_60)]\n",
    "    json_response = get_cehub_data(cehub_endpoint, trial_time[LATITUDE][i], trial_time[LONGITUDE][i], trial_time[TRIAL][i], trial_time[START_DATE_COLUMN][i], trial_time[END_DATE_COLUMN][i], soil_queries)\n",
    "    try:\n",
    "        response_dict = convert_soil_json_to_dict(json_response)\n",
    "        each_trial_df = pd.DataFrame(response_dict)\n",
    "        soil_df = soil_df.append(each_trial_df, ignore_index=True)\n",
    "    except Exception as exe:\n",
    "        print(f\"Not working for {i} and it failed with: {exe}\")\n",
    "        failed_trials += 1\n",
    "\n",
    "print(f'<{failed_trials}> trials has failed to retrive from CE Hub out of <{total_trial}>')\n",
    "soil_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd7707d",
   "metadata": {},
   "source": [
    "# Functions to Merge Trial Data with CE Hub Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff8b91a-dc3a-411e-b612-763f7ed900ea",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Basic Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "94a69ec2-8737-491c-9173-bb898279c385",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_column_names (candidates_column_names: List[str], columns_tobe_removed: List[str]) -> List[str]:\n",
    "    print(f'Candidate columns: \\n{candidates_column_names}')\n",
    "    #remove some of the columns\n",
    "\n",
    "    for i in range(len(columns_tobe_removed)):\n",
    "        candidates_column_names.remove(columns_tobe_removed[i])\n",
    "    print(f'\\nSelected columns: \\n{candidates_column_names}')\n",
    "    return candidates_column_names\n",
    "\n",
    "# Validates and calculates the experiement days\n",
    "def get_experiment_days(selected_df: pd, i: int, from_date_col, to_date_col, from_date_offset:int, to_date_offset:int) -> int:\n",
    "\n",
    "    start_date = selected_df[START_DATE_COLUMN][i]\n",
    "    end_date = selected_df[END_DATE_COLUMN][i]\n",
    "    \n",
    "    # We can be sure that the from and to date col data are within start_date and end_date range\n",
    "    from_date = selected_df[from_date_col][i]\n",
    "    to_date = selected_df[to_date_col][i]\n",
    "    \n",
    "    # A few validations\n",
    "    # 1. If any of these values is empty\n",
    "    if pd.isna(from_date):\n",
    "        from_date = start_date\n",
    "        if from_date_offset < 0: \n",
    "            from_date_offset = 0\n",
    "        print(f'from_date is empty, using start_date value <{from_date}>')\n",
    "    if pd.isna(to_date):\n",
    "        to_date = end_date\n",
    "        if to_date_offset > 0: \n",
    "            to_date_offset = 0\n",
    "        print(f'to_date is empty, using end_date value <{to_date}>')\n",
    "    \n",
    "    if from_date > to_date:\n",
    "        print(f'from_date <{from_date}> is later than to_date <{to_date}>, swapping the values')\n",
    "        from_date, to_date = to_date, from_date\n",
    "        print(f'from_date now is <{from_date}>, to_date is now <{to_date}> after swapping')\n",
    "    \n",
    "    # 2. If user sets the offset value too aggasively \n",
    "    from_date_delta = from_date + timedelta(days = from_date_offset)   \n",
    "    if from_date_delta < start_date or from_date_delta > end_date:\n",
    "        print(f'from_date_delta <{from_date_delta}> is outside the start_date <{start_date}> and end_date <{end_date}> range, use from_date <{from_date}> without offsets.')\n",
    "        from_date_delta = from_date \n",
    "    \n",
    "    to_date_delta = to_date + timedelta(days = to_date_offset)\n",
    "    if to_date_delta > end_date or to_date_delta < start_date:\n",
    "        print(f'to_date_delta <{to_date_delta}> is outside the start_date <{start_date}> and end_date <{end_date}> range, use to_date <{to_date}> without offsets.')\n",
    "        to_date_delta = to_date\n",
    "    \n",
    "    experiment_days: datetime.date \n",
    "    try:\n",
    "        experiment_days = (to_date_delta-from_date_delta) + timedelta(days = 1)\n",
    "    except Exception as exe:\n",
    "        print(f'exception at to_date: <{to_date}>, from_date: <{from_date}>')\n",
    "        print(f'exception at to_date_delta: <{to_date_delta}>, from_date_delta: <{from_date_delta}>')\n",
    "    \n",
    "    # 3. If the delta dates are overlapping\n",
    "    if experiment_days.days <= 0:\n",
    "        print(f'The experiment days is <= 0, use from_date <{from_date}> and to_date <{to_date}> without the offsets')\n",
    "        experiment_days = (to_date-from_date) + timedelta(days = 1)\n",
    "    \n",
    "    return experiment_days.days\n",
    "\n",
    "def get_current_time() :\n",
    "    current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "    print(\"Current Time =\", current_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669a026c-f25a-4522-8b71-012c612475dd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Merge Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b750d41b-a9ca-40fa-8c69-d006ac3c4bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge trial data with CE Hub weather data with flexibility\n",
    "def merge_weather_data_basic (column_names: List[str], df_tobe_merged: pd.DataFrame, cehub_data_df: pd.DataFrame,\n",
    "                              from_date_col:str, to_date_col:str, from_date_offset:int, to_date_offset: int,\n",
    "                              column_suffix:str, counter: int) -> pd.DataFrame :\n",
    "    additional_data = pd.DataFrame(columns = column_names)\n",
    "    selected_df = df_tobe_merged.reset_index(drop = True)    \n",
    "    \n",
    "    print(f'The {counter} time of merging...')\n",
    "    selected_df.info()\n",
    "    \n",
    "    for column in column_names:\n",
    "        averages = []\n",
    "        for i, trial in enumerate(selected_df.Trial):\n",
    "            # print(f'Merging for trial {trial} column {column}')\n",
    "            experiment_days = get_experiment_days(selected_df, i, from_date_col, to_date_col, from_date_offset, to_date_offset)\n",
    "            temp_sum = 0\n",
    "            all_values = cehub_data_df[column][cehub_data_df[TRIAL]==trial]\n",
    "            j = 0\n",
    "            added_values = 0\n",
    "            if (len(all_values)==0):\n",
    "                averages.append(float('nan'))\n",
    "                continue\n",
    "            for j, temp in enumerate(all_values):\n",
    "                if j == experiment_days:\n",
    "                    # we are done - got all the values we needed\n",
    "                    try:\n",
    "                        average = temp_sum / experiment_days\n",
    "                        averages.append(average)\n",
    "                    except Exception as exe:\n",
    "                        print(f'exception at column: {column}, trial id: {trial}, exe is {exe}')\n",
    "                    break\n",
    "                else:\n",
    "                    # keep adding temperatures until we have enough\n",
    "                    if temp is None: temp = 0\n",
    "                    temp_sum += temp\n",
    "                    added_values+=1\n",
    "            if j < experiment_days:\n",
    "                averages.append(temp_sum/(j+1))\n",
    "\n",
    "        selected_df[column] = averages\n",
    "        \n",
    "        if (len(column_suffix)>0):\n",
    "            new_column_name = column + '_' + column_suffix.lower()\n",
    "            selected_df.rename({column: new_column_name}, axis=1, inplace=True)\n",
    "            print(f'Renamed {column} to {new_column_name}')\n",
    "        \n",
    "    return selected_df\n",
    "\n",
    "def merge_weather_data (column_names: List[str], interested_dates_cols: list, df_tobe_merged: pd.DataFrame, cehub_data_df: pd.DataFrame) -> pd.DataFrame :\n",
    "       \n",
    "    # We use START_DATE and ASSMT_DATE for the default join\n",
    "    from_date_col:str = START_DATE_COLUMN\n",
    "    to_date_col:str = ASSMT_DATE\n",
    "    print('\\nStarted default merge')\n",
    "    get_current_time()\n",
    "    j = 0\n",
    "    merge_df = merge_weather_data_basic(column_names, df_tobe_merged, cehub_data_df, from_date_col, to_date_col, 0, 0, DEFAULT, j)\n",
    "    print('\\nFinished the default merge\\n')\n",
    "    \n",
    "    # Check the cehub.ini file to see if there are any [Weather_Merge_Additional_] sections specified by the user, \n",
    "    # reuse merge_weather_data_basic function above according to number of Weather_Merge_Additional_ provided\n",
    "    additional_merges: list = get_property_sections_with_regex(INI_FILE, WEATHER_MERGE_ADD_COL)\n",
    "    \n",
    "    for additional_merge in additional_merges:\n",
    "        j += 1\n",
    "        # get the from_date and to_date\n",
    "        from_date_col = get_property(INI_FILE, additional_merge, FROM_DATE)\n",
    "        to_date_col = get_property(INI_FILE, additional_merge, TO_DATE)\n",
    "        # get and validate the from and to date with offset values by comparing them to START and END dates\n",
    "        from_date_offset = int(get_property(INI_FILE, additional_merge, FROM_DATE_OFFSET))\n",
    "        to_date_offset = int(get_property(INI_FILE, additional_merge, TO_DATE_OFFSET))\n",
    "        \n",
    "        # adding column_suffix\n",
    "        column_suffix = additional_merge[len(WEATHER_MERGE_ADD_COL):]\n",
    "        \n",
    "        print(f'\\nStarted the {additional_merge} merge\\n')\n",
    "        get_current_time()\n",
    "        additional_merge_df = merge_weather_data_basic (column_names, merge_df, cehub_data_df, \n",
    "                                  from_date_col, to_date_col, from_date_offset, to_date_offset, column_suffix, j)\n",
    "        print('trying to copy additional_merge_df...')\n",
    "        merge_df = additional_merge_df.copy()\n",
    "        print(f'\\nFinished the {additional_merge} merge\\n')\n",
    "        \n",
    "    return merge_df # additional_merge_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0a9133-49f4-4aec-940e-667581b0bed0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Merge Soil Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9526a500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merges trial data or already merged trial-weather data with CE Hub soil data by using pyspark\n",
    "def merge_soil_data (main_df: pd.DataFrame, cehub_df: pd.DataFrame, columns_tobe_removed: List[str]) -> pd.DataFrame :\n",
    "    \n",
    "    #drop duplicated columns from soil data\n",
    "    cehub_dropped_df = cehub_df.drop(columns_tobe_removed, axis=1)\n",
    "    print(f'Soil columns after dropping the duplicated columns are: \\n{cehub_dropped_df.columns}') \n",
    "\n",
    "    # left join on two dataframes\n",
    "    merged_df = main_df.merge(cehub_dropped_df, on='Trial', how='left')\n",
    "\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc78f37",
   "metadata": {},
   "source": [
    "# Performs Merge and Outputs to Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f5f9e4",
   "metadata": {},
   "source": [
    "## Output Directory and Output File Paths\n",
    "1. Trial + Weather data with daily resolution: input_filename_weather_data.csv\n",
    "2. Trail + Soil data: input_filename_soil_data.csv\n",
    "3. Trial + Weather + Soil data: inputfilename_trial_weather_soil_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9024646d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory <C:\\example\\fields\\cehub\\output> already exists\n"
     ]
    }
   ],
   "source": [
    "# Checks if output directory exists, creates it if not\n",
    "isExist = os.path.exists(output_file_dir)\n",
    "\n",
    "if not isExist:\n",
    "    # Create a new directory because it does not exist \n",
    "    os.makedirs(output_file_dir)\n",
    "    print(f'Output directory <{output_file_dir}> does not exist, it is now created!')\n",
    "else:\n",
    "    print(f'Output directory <{output_file_dir}> already exists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d70079fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructs output file paths\n",
    "# trial data file name and output path\n",
    "trial_data_file_name = os.path.splitext(source_data_filename)[0]\n",
    "trial_data_file_name_path = f'{output_file_dir}{os.path.sep}{trial_data_file_name}'\n",
    "\n",
    "# weather data only\n",
    "weather_only_filepath = f'{trial_data_file_name_path}_weather_data_only.csv'\n",
    "# trial data with weather data filepath\n",
    "trial_weather_filepath = f'{trial_data_file_name_path}_weather_data.csv'\n",
    "# trial data with soil data filepath\n",
    "trial_soil_filepath = f'{trial_data_file_name_path}_soil_data.csv'\n",
    "# trial data with weather and soil data filepath\n",
    "trial_weather_soil_filepath = f'{trial_data_file_name_path}_weather_soil_data.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f277116",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Weather Data Only and Trial Data with Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f108d9e0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate columns: \n",
      "['Trial', 'Latitude', 'Longitude', 'Altitude', 'Dates', 'Relative_Humidity_(Max)_(%)', 'Relative_Humidity_(Min)_(%)', 'Relative_Humidity_(Mean)_(%)', 'Cloud_Cover_Total_(Mean)_(%)', 'Cloud_Cover_High_(Mean)_(%)', 'Cloud_Cover_Medium_(Mean)_(%)', 'Cloud_Cover_Low_(Mean)_(%)', 'Sunshine_Duration_(Sum)_(min)', 'Shortwave_Radiation_(Mean)_(W/mÂ²)', 'Direct_Shortwave_Radiation_(Mean)_(W/mÂ²)', 'Diffuse_Shortwave_Radiation_(Mean)_(W/mÂ²)', 'Evapotranspiration_(Sum)_(mm)', 'Soil_Temperature_(Max)_(Â°C)', 'Soil_Temperature_(Min)_(Â°C)', 'Soil_Temperature_(Mean)_(Â°C)', 'Soil_Moisture_(Max)_(mÂ³/mÂ³)', 'Soil_Moisture_(Min)_(mÂ³/mÂ³)', 'Soil_Moisture_(Mean)_(mÂ³/mÂ³)', 'Vapor_Pressure_Deficit_(Max)_(hPa)', 'Vapor_Pressure_Deficit_(Min)_(hPa)', 'Vapor_Pressure_Deficit_(Mean)_(hPa)', 'Temperature_(Max)_(Â°C)', 'Temperature_(Min)_(Â°C)', 'Temperature_(Mean)_(Â°C)', 'Precipitation_Total_(Sum)_(mm)', 'Wind_Speed_(Max)_(km/h)', 'Wind_Speed_(Min)_(km/h)', 'Wind_Speed_(Mean)_(km/h)', 'Wind_Direction_Dominant_(None)_(Â°)', 'UV_Radiation_(Mean)_(W/mÂ²)']\n",
      "\n",
      "Selected columns: \n",
      "['Altitude', 'Relative_Humidity_(Max)_(%)', 'Relative_Humidity_(Min)_(%)', 'Relative_Humidity_(Mean)_(%)', 'Cloud_Cover_Total_(Mean)_(%)', 'Cloud_Cover_High_(Mean)_(%)', 'Cloud_Cover_Medium_(Mean)_(%)', 'Cloud_Cover_Low_(Mean)_(%)', 'Sunshine_Duration_(Sum)_(min)', 'Shortwave_Radiation_(Mean)_(W/mÂ²)', 'Direct_Shortwave_Radiation_(Mean)_(W/mÂ²)', 'Diffuse_Shortwave_Radiation_(Mean)_(W/mÂ²)', 'Evapotranspiration_(Sum)_(mm)', 'Soil_Temperature_(Max)_(Â°C)', 'Soil_Temperature_(Min)_(Â°C)', 'Soil_Temperature_(Mean)_(Â°C)', 'Soil_Moisture_(Max)_(mÂ³/mÂ³)', 'Soil_Moisture_(Min)_(mÂ³/mÂ³)', 'Soil_Moisture_(Mean)_(mÂ³/mÂ³)', 'Vapor_Pressure_Deficit_(Max)_(hPa)', 'Vapor_Pressure_Deficit_(Min)_(hPa)', 'Vapor_Pressure_Deficit_(Mean)_(hPa)', 'Temperature_(Max)_(Â°C)', 'Temperature_(Min)_(Â°C)', 'Temperature_(Mean)_(Â°C)', 'Precipitation_Total_(Sum)_(mm)', 'Wind_Speed_(Max)_(km/h)', 'Wind_Speed_(Min)_(km/h)', 'Wind_Speed_(Mean)_(km/h)', 'Wind_Direction_Dominant_(None)_(Â°)', 'UV_Radiation_(Mean)_(W/mÂ²)']\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'interested_dates_cols' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9416/1098057462.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mtrial_weather_column_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_column_names\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mweather_column_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweather_columns_tobe_removed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m# merges trial data with weather data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mtrial_weather_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmerge_weather_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial_weather_column_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minterested_dates_cols\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrial_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweather_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;31m# writes the trail and weather merged data to file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mtrial_weather_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial_weather_filepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'interested_dates_cols' is not defined"
     ]
    }
   ],
   "source": [
    "# TODO: a flag to export with full name, but short name when merge with trial data\n",
    "# merges trial data with CE Hub weather data\n",
    "# defines the columns list for trial and weather data merge\n",
    "weather_column_names = list(weather_df)\n",
    "weather_columns_tobe_removed = [TRIAL, LATITUDE, LONGITUDE, DATES]\n",
    "\n",
    "# weather data only to csv file\n",
    "weather_df.to_csv(weather_only_filepath, index = False)\n",
    "\n",
    "# creates the column list to be iterated over\n",
    "trial_weather_column_names = get_column_names (weather_column_names, weather_columns_tobe_removed)\n",
    "# merges trial data with weather data\n",
    "trial_weather_df = merge_weather_data(trial_weather_column_names, interested_dates_cols, trial_df, weather_df)\n",
    "# writes the trail and weather merged data to file\n",
    "trial_weather_df.to_csv(trial_weather_filepath, index = False)\n",
    "# checks joined data columns\n",
    "print(f'\\nJoined trial and weather data columns are:\\n{trial_weather_df.columns}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fdfb9e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Trial Data with Soil Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0eb6ffae-ad71-4ae4-ba6f-5e150b4f3aed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trial_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9416/1387498212.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# merges trial data with CE Hub soil data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mtrial_soil_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmerge_soil_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msoil_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msoil_columns_tobe_removed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mtrial_soil_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial_soil_filepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'\\nJoined trial and soil data columns are:\\n{trial_soil_df.columns}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'trial_df' is not defined"
     ]
    }
   ],
   "source": [
    "# identifies the duplicated columns in the soil data that need to be removed\n",
    "soil_columns_tobe_removed = [LATITUDE, LONGITUDE]\n",
    "\n",
    "# merges trial data with CE Hub soil data\n",
    "trial_soil_df = merge_soil_data(trial_df, soil_df, soil_columns_tobe_removed)\n",
    "trial_soil_df.to_csv(trial_soil_filepath, index = False)\n",
    "print(f'\\nJoined trial and soil data columns are:\\n{trial_soil_df.columns}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239e0818",
   "metadata": {},
   "source": [
    "## Trial-Weather Data with Soil DataÂ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "1d2b1e24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soil columns after dropping the duplicated columns are: \n",
      "Index(['Trial', 'Bulk_Density_(0-30)_(kg/mÂ³)',\n",
      "       'Cation_Exchange_Capacity_(0-30)_(cmolc/kg)',\n",
      "       'Clay_Content_(0-2_micro_meter)_mass_fraction_(0-30)_(%)',\n",
      "       'Coarse_Fragments_volumetric_fraction_(0-30)_(vol. %)',\n",
      "       'Organic_Carbon_Content_(fine_earth_fraction)_(0-30)_(%)',\n",
      "       'Organic_Carbon_Density_(0-30)_(kg/mÂ³)',\n",
      "       'Organic_Carbon_Stocks_(0-30 cm)_(kg/mÂ²)',\n",
      "       'Sand_content_(50-2000_micro_meter)_mass_fraction_(0-30)_(%)',\n",
      "       'Silt_Content_(2-50_micro_meter)_mass_fraction_(0-30)_(%)',\n",
      "       'Total_Nitrogen_Content_(0-30)_(g/kg)', 'pH_in_H2O_(0-30)_(pH)',\n",
      "       'Bulk_Density_(0-60)_(kg/mÂ³)',\n",
      "       'Cation_Exchange_Capacity_(0-60)_(cmolc/kg)',\n",
      "       'Clay_Content_(0-2_micro_meter)_mass_fraction_(0-60)_(%)',\n",
      "       'Coarse_Fragments_volumetric_fraction_(0-60)_(vol. %)',\n",
      "       'Organic_Carbon_Content_(fine_earth_fraction)_(0-60)_(%)',\n",
      "       'Organic_Carbon_Density_(0-60)_(kg/mÂ³)',\n",
      "       'Sand_content_(50-2000_micro_meter)_mass_fraction_(0-60)_(%)',\n",
      "       'Silt_Content_(2-50_micro_meter)_mass_fraction_(0-60)_(%)',\n",
      "       'Total_Nitrogen_Content_(0-60)_(g/kg)', 'pH_in_H2O_(0-60)_(pH)'],\n",
      "      dtype='object')\n",
      "\n",
      "Joined trial, weather and soil data columns are:\n",
      "Index(['Master_Prt', 'Derived_Prt', 'Trial', 'Trial_Year', 'Country_Code',\n",
      "       'State_province_code', 'State_province_name', 'City', 'Site_Type',\n",
      "       'Syngenta_FieldScientist',\n",
      "       ...\n",
      "       'Bulk_Density_(0-60)_(kg/mÂ³)',\n",
      "       'Cation_Exchange_Capacity_(0-60)_(cmolc/kg)',\n",
      "       'Clay_Content_(0-2_micro_meter)_mass_fraction_(0-60)_(%)',\n",
      "       'Coarse_Fragments_volumetric_fraction_(0-60)_(vol. %)',\n",
      "       'Organic_Carbon_Content_(fine_earth_fraction)_(0-60)_(%)',\n",
      "       'Organic_Carbon_Density_(0-60)_(kg/mÂ³)',\n",
      "       'Sand_content_(50-2000_micro_meter)_mass_fraction_(0-60)_(%)',\n",
      "       'Silt_Content_(2-50_micro_meter)_mass_fraction_(0-60)_(%)',\n",
      "       'Total_Nitrogen_Content_(0-60)_(g/kg)', 'pH_in_H2O_(0-60)_(pH)'],\n",
      "      dtype='object', length=151)\n"
     ]
    }
   ],
   "source": [
    "trial_weather_soil_df = merge_soil_data(trial_weather_df, soil_df, soil_columns_tobe_removed)\n",
    "trial_weather_soil_df.to_csv(trial_weather_soil_filepath, index = False)\n",
    "print(f'\\nJoined trial, weather and soil data columns are:\\n{trial_weather_soil_df.columns}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f03ec51d-e624-4879-accb-0afbcbf41f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge Weather and Soil data\n",
    "#Hack for trials with 400 502 responses.\n",
    "soil_columns_tobe_removed = [LATITUDE, LONGITUDE]\n",
    "soil_dropped_df = soil_df.drop(soil_columns_tobe_removed, axis=1) \n",
    "\n",
    "# left join on two dataframes\n",
    "merged_df = weather_df.merge(soil_dropped_df, on='Trial', how='left')\n",
    "merged_df.to_csv(trial_weather_soil_filepath, index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
